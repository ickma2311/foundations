{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§® Exercise: PCA and Linear Algebra â€” Exercises\n",
    "\n",
    "**Prerequisites**: Read `pca_theory.md` for key concepts.\n",
    "\n",
    "These exercises help you implement PCA and understand its connection to linear algebra in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic PCA Implementation\n",
    "\n",
    "You are given a dataset of 2D points (could represent activations from a neural network layer):\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "2.5 & 2.4 \\\\\n",
    "0.5 & 0.7 \\\\\n",
    "2.2 & 2.9 \\\\\n",
    "1.9 & 2.2 \\\\\n",
    "3.1 & 3.0 \\\\\n",
    "2.3 & 2.7 \\\\\n",
    "2.0 & 1.6 \\\\\n",
    "1.0 & 1.1 \\\\\n",
    "1.5 & 1.6 \\\\\n",
    "1.1 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Tasks\n",
    "1. **Center the data**  \n",
    "   Subtract the mean of each column from the dataset $X$.\n",
    "\n",
    "2. **Compute the covariance matrix** of the centered data.\n",
    "\n",
    "3. **Find the eigenvalues and eigenvectors** of the covariance matrix.  \n",
    "   - Which eigenvector corresponds to the largest eigenvalue?  \n",
    "   - Interpret this as the **direction of maximum variance**.\n",
    "\n",
    "4. **Project the data** onto the first principal component (1D subspace).  \n",
    "   Write the new transformed dataset.\n",
    "\n",
    "5. (Optional, Deep Learning connection)  \n",
    "   - Explain why PCA can be seen as a **linear transformation** that learns a new basis.  \n",
    "   - Discuss how this relates to **hidden layers in neural networks**, which also learn useful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Center the data\n",
    "X = np.array([\n",
    "    [2.5, 2.4],\n",
    "    [0.5, 0.7],\n",
    "    [2.2, 2.9],\n",
    "    [1.9, 2.2],\n",
    "    [3.1, 3.0],\n",
    "    [2.3, 2.7],\n",
    "    [2.0, 1.6],\n",
    "    [1.0, 1.1],\n",
    "    [1.5, 1.6],\n",
    "    [1.1, 0.9]\n",
    "])\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Step 2: Covariance matrix\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Step 3: Eigen decomposition\n",
    "eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Step 4: Project onto the first principal component\n",
    "pc1 = eigvecs[:, np.argmax(eigvals)]\n",
    "X_pca = X_centered @ pc1\n",
    "\n",
    "print(\"X_pca:\",X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the center\n",
    "X_centered=X-np.mean(X,axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "\n",
    "X_cov=np.cov(X_centered,rowvar=False)\n",
    "\n",
    "# calculate the eigen values and eigen vectors\n",
    "\n",
    "eigvals,eigvecs=np.linalg.eig(X_cov)\n",
    "\n",
    "# Project the data on to the first principal component\n",
    "\n",
    "pc1=eigvecs[:,np.argmax(eigvals)]\n",
    "\n",
    "X_pca=X_centered@pc1\n",
    "\n",
    "print(\"X_pca:\",X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Real-World PCA\n",
    "\n",
    "Now let's apply PCA to real-world datasets to understand how dimensionality reduction preserves important features while removing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints for Real-World PCA Exercise\n",
    "\n",
    "**Dataset Options:**\n",
    "1. **Wine Dataset**: `from sklearn.datasets import load_wine` (13 features, 3 classes)\n",
    "2. **Breast Cancer**: `from sklearn.datasets import load_breast_cancer` (30 features, 2 classes)\n",
    "3. **Digits**: `from sklearn.datasets import load_digits` (64 features, 10 classes)\n",
    "\n",
    "**Steps to implement:**\n",
    "1. Load dataset and examine features\n",
    "2. Standardize the data using `StandardScaler()`\n",
    "3. Apply manual PCA using eigendecomposition\n",
    "4. Use `np.argsort(eigenvals)[-k:][::-1]` to select top k components\n",
    "5. Visualize results and calculate variance explained\n",
    "6. Verify that main features/classes are preserved after dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [\n",
    "# TODO: Implement PCA on a real-world dataset\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load a dataset (choose one from the hints above)\n",
    "X=load_wine().data\n",
    "\n",
    "# Step 2: Standardize the features \n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Center the data\n",
    "X_centered=X_scaled-np.mean(X_scaled,axis=0)\n",
    "\n",
    "# Step 4: Compute covariance matrix\n",
    "X_cov=np.cov(X_centered,rowvar=False)\n",
    "\n",
    "# Step 5: Eigendecomposition\n",
    "eigvals,eigvecs=np.linalg.eig(X_cov)\n",
    "\n",
    "\n",
    "# Step 6: Select top k components using np.argsort()\n",
    "\n",
    "pc=eigvecs[:,np.argsort(eigvals)[-3:][::-1]]\n",
    "\n",
    "# Step 7: Project data onto principal components\n",
    "X_pca=X_centered@pc\n",
    "\n",
    "# Step 8: Calculate variance explained\n",
    "print(\"Explained variance:\",eigvals[-2:][::-1]/np.sum(eigvals[-2:][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Visualize results (scatter plot, eigenvalues, etc.)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1],c=load_wine().target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Analysis Questions\n",
    "\n",
    "Answer the following questions based on your PCA implementation:\n",
    "\n",
    "1. **Variance Preservation**: What percentage of the total variance is captured by the first two principal components?\n",
    "\n",
    "2. **Dimensionality Reduction**: How many dimensions do you need to capture 95% of the variance?\n",
    "\n",
    "3. **Class Separation**: In the 2D PCA visualization, can you still distinguish between different classes?\n",
    "\n",
    "4. **Deep Learning Connection**: How might PCA be used as a preprocessing step before training a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Answer the analysis questions with code and explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}