{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Chapter 4: Numerical Computation\n",
    "\n",
    "Deep learning algorithms depend heavily on numerical computation to solve optimization problems that cannot be solved analytically. This chapter covers the fundamental numerical methods and considerations essential for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Knowledge Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importance of Numerical Computation\n",
    "- Deep learning heavily relies on numerical methods since most problems cannot be solved analytically.\n",
    "- Focus on stability, efficiency, and acceptable approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numerical Precision and Stability\n",
    "- Floating-point representation (finite precision, rounding error).\n",
    "- Overflow and underflow.\n",
    "- Ill-conditioned problems (condition number).\n",
    "- Numerical stability and error propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Computation\n",
    "- Numerical gradients (finite difference approximation).\n",
    "- Symbolic differentiation.\n",
    "- Automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Iterative Optimization Methods\n",
    "- Gradient Descent (GD).\n",
    "- Stochastic Gradient Descent (SGD).\n",
    "- Batch vs. Mini-batch updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hessian and Second-Order Information\n",
    "- Newton's Method.\n",
    "- Conjugate Gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Constrained Optimization\n",
    "- Projected Gradient Descent.\n",
    "- Lagrangian multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Numerical Tricks in Deep Learning\n",
    "- Avoiding vanishing/exploding gradients.\n",
    "- Numerically stable softmax and log-sum-exp trick.\n",
    "- Parameter initialization considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Explain why deep learning relies more on numerical approximation than analytic solutions.  \n",
    "üëâ Hint: Consider model size, nonlinearity, and parameter dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your explanation here as a comment or markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Given matrix  \n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4.0001 \\end{bmatrix}\n",
    "$$  \n",
    "Compute its condition number and explain why it is ill-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement condition number calculation\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], \n",
    "              [2, 4.0001]])\n",
    "\n",
    "# Calculate condition number using np.linalg.cond()\n",
    "# Explain why this matrix is ill-conditioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numerical Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Using finite difference approximation, compute the derivative of  \n",
    "$$\n",
    "f(x) = x^3 - 3x^2 + 2x\n",
    "$$  \n",
    "at $x = 2$, and compare it with the analytical derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement finite difference approximation\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**3 - 3*x**2 + 2*x\n",
    "\n",
    "def analytical_derivative(x):\n",
    "    # TODO: Calculate analytical derivative f'(x) = 3x^2 - 6x + 2\n",
    "    pass\n",
    "\n",
    "def finite_difference(f, x, h=1e-5):\n",
    "    # TODO: Implement (f(x+h) - f(x-h)) / (2*h)\n",
    "    pass\n",
    "\n",
    "x = 2\n",
    "# Compare numerical vs analytical derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Perform **Gradient Descent** manually:  \n",
    "- Function: $f(x) = (x-3)^2$  \n",
    "- Initial point: $x_0 = 0$  \n",
    "- Learning rate: $\\eta = 0.1$  \n",
    "- Compute the first 3 iterations of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent manually\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (x - 3)**2\n",
    "\n",
    "def df_dx(x):\n",
    "    # TODO: Calculate derivative f'(x) = 2(x-3)\n",
    "    pass\n",
    "\n",
    "# Initial conditions\n",
    "x = 0\n",
    "learning_rate = 0.1\n",
    "\n",
    "# TODO: Perform 3 iterations of gradient descent\n",
    "# x_new = x_old - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Write down the main differences between **Stochastic Gradient Descent (SGD)** and **Batch Gradient Descent (BGD)**. Provide suitable application scenarios for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your comparison here as comments or markdown cell\n",
    "# Consider:\n",
    "# - Computational cost per iteration\n",
    "# - Memory requirements\n",
    "# - Convergence properties\n",
    "# - Noise in gradient estimates\n",
    "# - When to use each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Second-Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Apply Newton's Method to:  \n",
    "- Function: $f(x) = x^2 - 2$  \n",
    "- Initial point: $x_0 = 1$  \n",
    "- Perform 2 iterations and compare with the true solution $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Newton's method\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 2\n",
    "\n",
    "def f_prime(x):\n",
    "    # TODO: Calculate f'(x) = 2x\n",
    "    pass\n",
    "\n",
    "def f_double_prime(x):\n",
    "    # TODO: Calculate f''(x) = 2\n",
    "    pass\n",
    "\n",
    "# Newton's method: x_new = x_old - f'(x) / f''(x)\n",
    "x = 1\n",
    "true_solution = np.sqrt(2)\n",
    "\n",
    "# TODO: Perform 2 iterations and compare with true solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numerical Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Why is softmax usually computed as:  \n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\n",
    "$$  \n",
    "instead of directly $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement both versions and demonstrate numerical issues\n",
    "import numpy as np\n",
    "\n",
    "def naive_softmax(x):\n",
    "    # TODO: Implement direct softmax\n",
    "    pass\n",
    "\n",
    "def stable_softmax(x):\n",
    "    # TODO: Implement numerically stable softmax\n",
    "    pass\n",
    "\n",
    "# Test with large values that cause overflow\n",
    "x = np.array([1000, 1001, 1002])\n",
    "\n",
    "# TODO: Compare results and explain the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overflow and Underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "Run the following code in Python / NumPy and explain the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this code and explain the results\n",
    "import numpy as np\n",
    "\n",
    "print(\"exp(1000) =\", np.exp(1000))     # Test overflow\n",
    "print(\"exp(-1000) =\", np.exp(-1000))   # Test underflow\n",
    "\n",
    "# TODO: Explain what overflow and underflow mean\n",
    "# TODO: Discuss implications for deep learning computations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}