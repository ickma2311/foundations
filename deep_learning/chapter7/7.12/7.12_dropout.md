# 7.12 Dropout

## Limitation of Bagging
When training a very large neural network, it is often impractical to train and average multiple models because the computational cost is too high.

## Dropout
Dropout is a computationally efficient alternative to bagging that trains an ensemble of subnetworks by randomly dropping units during training.

If we have n droppable units, each of them can be either kept or dropped independently, we have $2^n$ sub networks.

$$
2\times2\times2\text{...}\times2=2^n
$$

![image](https://prod-files-secure.s3.us-west-2.amazonaws.com/31da91d7-545c-462b-9dc4-8c7611008c0e/b724fada-8daf-45f1-ac53-d144ec37af00/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466RTGIBD75%2F20251031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251031T035756Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEMaCXVzLXdlc3QtMiJIMEYCIQDttduTf9lkolYFsTARVzzBhYJuIsvCoRkK2e3xjl9hMwIhAK4vTgqmMIKoTKajcmi8D87dpHcU3ltxluZbY8mBWX1TKogECPz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgwNnLHZWMAxPnIAnKMq3ANcyygeXRHvbmIIRPVZD0JkZ4xQHU2FvBTJf5qKNBuMp0NEWW2JkvNs6FniuTvRiBQUhuf74zPvMDZLWb7pB305phynk8jjmx0fss0LyFXg0j3GdURnHydLdK6QVBf53%2Bf39ZeYH6b3Ih3392YtwL24s6mpgTQXreKQBTWrkv7MFq%2FuzCpimwSLsV%2BZC%2BCr3TC2m0lCGV%2BG5yCwKd8fcxKCcm8tFFU6o%2B4Ilw%2F3eaTbR%2Fu3gG1F12Uc1Oq%2Be8COvbi2A4EVffYNpHootyZNJnAV%2Fl%2Ffb5cKx8DYyNxlwuHLWcHrSO8B9lE2qr85P%2B7JrHRkBvGlF6ugxUYevp%2BWb9m1WsTcG1PTVzE1snKhFSEb15D9iFDGa%2BZNRblCvmXY2vS4re3paf7bWA5rU2qinOXCQkMNwbU8k41LzceTed5M%2BFpoXC3sPDpHzhr3OkeeiqrjcmgZWWbzHwwXu%2FkncNHVhL4HV3ENoqiacWhNXTOtwHTU4aJaPAUZZh53z15%2BA77ceM1iSPI1GZ%2FF1LFgfnaSV%2FBkIBmcCYIQmCbtkpElxpMF859s4IlxcHtbP1335Ff7y3vo5BgVLuf5sJK9eZMG0AnB7A0nAf9ug20PyNoP8gJq2I2SBxf%2BJHwsQzC0yZDIBjqkAUb6kXIouzV3dG%2BjUf3k3pEBQFS9ipsJ%2BE2Z281rwQWUgX%2FRDKQyoeJSM8eTxEbpUIvEJkFItYWx6rdvG07E2aeqbB%2FbQeHxZcYnIJlsy7H4rfABOizMpbF1Oel8SMQJxJNXc%2FWuqBZwhDrwGnifxTWgzHekydr%2FEToeMklj4OIbNWxCsg1BUq9Wq%2FfIqpmodgz5w3OiG2FV%2Bz4mnjfVmEIAuzC0&X-Amz-Signature=25881f09a255b4fdcef59bf0282bf2bdfa6bd5cbf15d23d36d07931da514d505&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

## Comparing with Bagging
Assume our task is to output the probability.
- 7.52
Bagging averages predictions from k independently trained models.

$$
\frac{1}{k}\sum_{i=1}^kp^{(i)}(y|x)
$$

- 7.53
Dropout takes a weighted sum over all possible mask configurations.
$p(\mu)$ is the proability to sample $\mu$

$$
\sum_\mu p(\mu)p(y|x,\mu)
$$

## mask
we use a vector to represent masks for each unit:

$$
\mu=[\mu_1,\mu_2,...\mu_n]
$$

During training, we sample masks,

$$
h=[h_1,h_2,...,h_n]\\
h'=h\odot\mu
$$

say $\mu=[1,0,1]$, then $h'=[h_1,0,h_2]$

### **Feasibility of Simple Forward Propagation in Dropout Inference**
Use geometric average, we get
7.54

$$
\tilde{p}_{ensemble}(y|x)=\sqrt[2^d]{\prod_{\mu}p(y|x,\mu)}
$$

7.55 Normalization
we can assume the distribution is even, and we get
7.55

$$
p_{ensemble}(y|x)=\frac{\tilde{p}(y|x)}{\sum_{y'}\tilde{p}(y'|x)}
$$

- we use the output times the probability(0.5) as the approximate output expectation.
For model families that don't have non-linear hidden unit, this assertion is accurate.
7.56
This is the standard softmax output for a model without dropout.

$$
P(y=y|v)=\text{softmax}(W^\top v+b)_y
$$

7.57
Introduce mask

$$
P(y=y|\mathbf{v};\mathbf{d})=\text{softmax}(\mathbf{W}^\top (\mathbf{d}\odot\mathbf{v})+b)_y \tag{7.57}
$$

$$
P_{\text{ensemble}}(y=y|\mathbf{v})=\frac{\tilde{P}_{\text{ensmeble}(y=y|\mathbf{v})}}{\sum_{y'}\tilde{P}_{\text{ensemble}(y=y|\mathbf{x}
)}} \tag{7.58}
$$

Use geometric average

$$
\tilde{P}_{\text{ensemble}}(y=y\mid\mathbf{v})=\sqrt[2^n]{\prod_{d\in\{0,1\}^n}P(y=y\mid \mathbf{v};d)} \tag{7.59}
$$

$$
\tilde{P}_{\mathrm{ensemble}}(y = y \mid \mathbf{v})
=
\sqrt[2^n]{\prod_{\mathbf{d} \in {0,1}^n} P(y = y \mid \mathbf{v}; \mathbf{d})} \tag{7.60}
$$

$$
\tilde{P}_{\text{ensemble}}(y|\mathbf{v})

\sqrt[2^n]{\prod_{\mathbf{d}\in\{0,1\}^n}
\mathrm{softmax}\!\big(\mathbf{W}^\top(\mathbf{d}\odot\mathbf{v})+\mathbf{b}\big)_y}
\tag{7.61}
$$

$$
\tilde{P}_{\text{ensemble}}(y|\mathbf{v})

\sqrt[2^n]{\prod_{\mathbf{d} \in \{0,1\}^n}
\frac{
\exp(\mathbf{W}y'^\top(\mathbf{d} \odot\mathbf{v})+b_y)
}{
\sum{y'}\exp(\mathbf{W}y'^\top(\mathbf{d}\odot\mathbf{v})+b{y'})
}
}
\tag{7.62}
$$

$$
\tilde{P}_{\mathrm{ensemble}}(y \mid \mathbf{v})

\frac{
\sqrt[2^n]{\displaystyle \prod_{\mathbf{d}_{\in{0,1}^n}}
\exp\big(\mathbf{W}{y}^{\top}(\mathbf{d}\odot \mathbf{v}) + b_y\big)}
}{
\sqrt[2^n]{\displaystyle \prod_{\mathbf{d}_{\in{0,1}^n}}
\sum_{y'} \exp\big(\mathbf{W}{y'}^{\top}(\mathbf{d}\odot \mathbf{v}) + b{y'}\big)}
}
\tag{7.63}
$$

Property of exponential function:

$$
\prod_ie^{a_i}=e^{\sum_i a_i}
$$

$$
\sqrt[k]{\prod_i^k x_i}=(\prod_i^k)^{\frac{1}{k}}=\exp(\frac{1}{k}\sum_i\log x_i)
$$

Because the denominator of 7.63 is static, so

$$
\tilde{P}_{\text{ensemble}}(y=y|v) \propto \sqrt[2n]{\prod_{d \in \{0,1\}^n} \exp(\mathbf{W}y^\top(d\odot v)+b_y)} \tag{7.64}
$$

use the geometry average property, we can get

$$
\exp(\frac{1}{2n}\exp{\log(\sum_{d \in \{0,1\}^n }W^\top(d\odot v)+b_y)}) =\\
\exp(\frac{1}{2n}(\sum_{d \in \{0,1\}^n }W^\top(d\odot v)+b_y)) \tag{7.65}
$$

$$
\exp(\frac{1}{2}W^\top v+b_y) \tag{7.66}
$$

This shows that at inference, we can simply scale weights by the keep probability instead of sampling multiple masks.

**Intuition**: Each unit will be either hidden or not with probability 0.5, so the expected input is 0.5 times the forward pass of all units. Therefore, multiplying by 0.5 at inference approximates the ensemble average.

## Computational Efficiency of Dropout Compared to Explicit Ensemble
Dropout acts as an implicit ensemble, where all subnetworks share the same parameters within one network.
During training, each unit has a probability (e.g. 0.5) of being active, so all subnetworks are trained efficiently within a single forward/backward pass.
During inference, only one forward pass is required â€” we simply multiply the activations (or equivalently the weights) by the keep probability (e.g. 0.5).
Gal and Ghahramani (2015) found that some models can achieve better classification accuracy by using Monte Carlo approximation with around 20 dropout samples. The optimal number of samples for inference approximation appears to be problem-dependent.

Dropout outperforms traditional low-cost regularization methods (e.g., weight decay, norm or sparsity constraints) and can be combined with them for additional gains.

## Limitations of Dropout
1. Requires a sufficiently large model capacity.
	Dropout is most effective when the network has enough parameters to compensate for the random removal of units. Small models may underfit when dropout is applied.
2. May be less effective with small training datasets.
	When the dataset is small, the stochastic noise introduced by dropout can overwhelm the learning signal, leading to unstable training or degraded performance.

## Intuition and Insights behind Dropout  

Dropout forces each unit to perform well independently, without relying on the presence of specific other units.
This encourages the network to learn redundant yet complementary representations,
so that every subnetwork formed during dropout can perform reasonably well.
As a result, combining many of these "good-enough" subnetworks produces a more powerful ensemble

---
Inspired by biological evolution, Hinton proposed that dropout resembles the process of gene exchange between organisms.
Evolutionary pressure not only rewards strong genes but also favors genes that remain effective after recombination.
Similarly, dropout encourages units to learn features that are robust to co-adaptation and can function well under many combinations.

---
By randomly "corrupting" its own input during training, dropout teaches the network to adapt to noise and missing information.
This adaptive destruction mechanism leads to features that are more stable and robust to input perturbations and unseen conditions.
