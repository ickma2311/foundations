# 7.3 Regularization and Under-Constrained Problems

## Problem Context

**Under-constrained scenarios:**
- $m < n$ (fewer samples than dimensions)
- Feature-dependent columns (linearly dependent features)

**Consequence**: Matrix $X^T X$ is not invertible, preventing direct solution of linear regression.

---

## Linear Regression and $X^T X$ Inverse

### Objective Function

Minimize the squared error:
$$
\min_w ||Xw - y||^2
$$

### Loss Function

$$
J(w; X, y) = ||Xw - y||^2 = (Xw - y)^T (Xw - y)
$$

where $||Xw - y||$ is the L2 norm of the residual.

---

## Derivation of the Normal Equations

### Step 1: Expand the Loss

$$
J(w; X, y) = (Xw)^T Xw - (Xw)^T y - y^T Xw + y^T y
$$

### Step 2: Simplify Using Vector Dot Product Symmetry

Since $Xw$ and $y$ are vectors, their dot product is commutative:
$$
(Xw)^T y = y^T Xw \quad \text{(because } \vec{v}_1 \cdot \vec{v}_2 = \vec{v}_2 \cdot \vec{v}_1\text{)}
$$

Therefore:
$$
J(w; X, y) = w^T X^T Xw - 2y^T Xw + y^T y
$$

### Step 3: Compute the Gradient

We need $\nabla_w J(w; X, y)$. The gradient has two parts:

**Part 1**: $w^T X^T Xw$

This is a quadratic form. Using the matrix derivative rule $\nabla_w (w^T A w) = 2Aw$ for symmetric $A$:
$$
\nabla_w (w^T X^T Xw) = 2X^T Xw
$$

**Detailed derivation:**
$$
\begin{aligned}
w^T X^T Xw &= (Xw)^T (Xw) = ||Xw||^2 \\
\nabla_w ||Xw||^2 &= 2X^T Xw
\end{aligned}
$$

**Part 2**: $-2y^T Xw$

This is a linear term. Using the rule $\nabla_w (a^T w) = a$:
$$
\nabla_w (-2y^T Xw) = -2X^T y
$$

**Explanation**:
- If $X$ is $m \times n$ and $w$ is $n \times 1$, then $Xw$ is $m \times 1$
- $y$ is $m \times 1$
- For the gradient with respect to $w$ (which is $n \times 1$), we need $X^T$ (which is $n \times m$) to match dimensions
- The linear derivative formula $\nabla_w (a^T w) = a$ tells us the gradient is the transpose of the coefficient

**Part 3**: $y^T y$ is constant with respect to $w$, so its gradient is 0.

### Step 4: Complete Gradient

$$
\nabla_w J(w; X, y) = 2X^T Xw - 2X^T y
$$

### Step 5: Solve for Optimal $w$

Set the gradient to zero:
$$
\begin{aligned}
2X^T Xw - 2X^T y &= 0 \\
X^T Xw &= X^T y \\
w &= (X^T X)^{-1} X^T y \quad \text{(if } X^T X \text{ is invertible)}
\end{aligned}
$$

---

## Regularization for Non-Invertible $X^T X$

When $X^T X$ is **not invertible** (under-constrained or rank-deficient), we use **regularization**:

### Ridge Regression (L2 Regularization)

Add a small positive constant to the diagonal:
$$
w = (X^T X + \alpha I_n)^{-1} X^T y
$$

where:
- $\alpha > 0$ is the regularization parameter
- $I_n$ is the $n \times n$ identity matrix

**Effect**: $X^T X + \alpha I_n$ is always invertible for $\alpha > 0$, even when $X^T X$ is singular.

### Moore-Penrose Pseudo-Inverse

The limiting case as $\alpha \to 0$:
$$
X^+ = \lim_{\alpha \to 0} (X^T X + \alpha I_n)^{-1} X^T
$$

This is the **pseudo-inverse** of $X$, which exists even when $X^T X$ is singular.

**Properties**:
- When $X^T X$ is invertible: $X^+ = (X^T X)^{-1} X^T$ (standard inverse)
- When $X^T X$ is singular: $X^+$ provides the minimum-norm solution

---

## Intuitive Derivation: From Scalar to Matrix

### Scalar Case

For a simple quadratic loss $(ax - c)^2$ where $a$ is input, $x$ is weight, and $c$ is target:

**Expand:**
$$
\text{Loss} = a^2 x^2 - 2acx + c^2
$$

**Derivative:**
$$
\frac{\partial \text{Loss}}{\partial x} = 2a^2 x - 2ac
$$

**Set to zero:**
$$
2a^2 x = 2ac \quad \Rightarrow \quad x = \frac{c}{a}
$$

### Extension to Matrix Form

By analogy, replace:
- $a^2 \to X^T X$ (scalar squared becomes matrix product)
- $x \to w$ (scalar weight becomes weight vector)
- $ac \to X^T y$ (scalar product becomes matrix-vector product)

This gives:
$$
\nabla_w J(w) = 2X^T Xw - 2X^T y
$$

**Key insight**: The matrix derivative follows the same pattern as the scalar derivative, with appropriate transpose operations to maintain dimensional consistency.

---

## Summary

| Scenario | $X^T X$ Status | Solution Method |
|----------|---------------|-----------------|
| $m \geq n$, full rank | Invertible | $w = (X^T X)^{-1} X^T y$ |
| $m < n$ (under-constrained) | Not invertible | $w = (X^T X + \alpha I_n)^{-1} X^T y$ |
| Rank deficient | Not invertible | Use pseudo-inverse $X^+$ |

**Key principle**: Regularization $X^T X + \alpha I_n$ ensures invertibility by adding a small positive value to all eigenvalues of $X^T X$.

---

*Source: Deep Learning Book, Chapter 7.3*
