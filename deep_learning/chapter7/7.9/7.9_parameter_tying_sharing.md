# 7.9 Parameter Tying and Parameter Sharing

## 1. Parameter Tying

**Definition**: Constraining parameters of different models or layers to be similar by adding a penalty term to the loss function.

**Mathematical form**:

$$
\Omega(w^{(A)}, w^{(B)}) = \|w^{(A)} - w^{(B)}\|_2^2
$$

**How it works**:
- Add this penalty term to the loss function
- Forces two models (or layers) to learn similar parameters
- The parameters are still independent, but regularization encourages similarity

**Effect**: Soft constraint that allows some deviation while encouraging parameter alignment.

---

## 2. Parameter Sharing

**Definition**: Using the exact same set of parameters across multiple locations or time steps.

**CNN example**: The same kernel (set of weights) is applied across all spatial locations of the input.

**Benefits**:
1. **Pattern detection**: Detects the same pattern (e.g., edge or texture) anywhere in the image
2. **Parameter reduction**: Dramatically reduces the number of parameters
3. **Translation equivariance**: Output shifts when input shifts

**Effect**: Hard constraint where parameters are identical by design, not just similar.

---

## 3. Parameter Sharing vs. Parameter Tying — Real-World Examples

**Note**: The following table is generated by ChatGPT.

### Parameter Sharing

| Model / Example | Mechanism | Mathematical Form | Purpose / Effect | Representative Paper |
|-----------------|-----------|-------------------|------------------|---------------------|
| **CNN (Convolutional Neural Network)** | Same kernel slides across all spatial positions | $y_{i,j} = \sum_{u,v} w_{u,v} x_{i+u, j+v}$ | Detect same pattern anywhere; reduce parameters; translation equivariance | LeCun et al., 1998 — *LeNet* |
| **RNN / LSTM / GRU** | Same weights used at each time step | $h_t = f(W_h h_{t-1} + W_x x_t)$ | Temporal consistency; handle variable-length sequences | Hochreiter & Schmidhuber, 1997 |
| **Transformer (ALBERT)** | All encoder layers share parameters | $\theta_1 = \theta_2 = \dots = \theta_L$ | Reduce memory; efficient deep sharing | Lan et al., 2020 — *ALBERT* |
| **Siamese / Twin Networks** | Two (or more) branches share all parameters | $f_\theta(x_1), f_\theta(x_2)$ | Compare similarity; representation consistency | Bromley et al., 1993 — *Siamese Nets* |

### Parameter Tying

| Model / Example | Mechanism | Mathematical Form | Purpose / Effect | Representative Paper |
|-----------------|-----------|-------------------|------------------|---------------------|
| **Word Embedding Tying (LMs, Transformers)** | Input embedding matrix and output softmax matrix tied | $W_{\text{out}} = E^T$ | Reduce parameters; consistent embedding space | Press & Wolf, 2017 |
| **Autoencoder** | Decoder weights tied to encoder transpose | $W_{\text{dec}} = W_{\text{enc}}^T$ | Regularize; stabilize training; mimic PCA | Hinton & Salakhutdinov, 2006 |
| **Multi-task Learning** | Different tasks' parameters constrained to be similar | $\Omega = \|w^{(A)} - w^{(B)}\|^2$ | Encourage knowledge sharing between tasks | Caruana, 1997 — *Multitask Learning* |
| **Knowledge Distillation** | Student layers tied to teacher via loss constraint | $\|h_s^{(l)} - h_t^{(l)}\|^2$ | Transfer intermediate representations | Sanh et al., 2019 — *DistilBERT* |

---

## 4. Key Differences

### Parameter Sharing
- **Nature**: Hard constraint — parameters are literally the same
- **Implementation**: Single parameter set used multiple times
- **Examples**: CNNs (spatial sharing), RNNs (temporal sharing)
- **Effect**: Exact reuse of weights

### Parameter Tying
- **Nature**: Soft constraint — parameters encouraged to be similar
- **Implementation**: Multiple parameter sets with similarity penalty
- **Examples**: Multi-task learning, knowledge distillation
- **Effect**: Parameters can differ but are regularized toward similarity

---

*Source: Deep Learning Book, Chapter 7.9*
