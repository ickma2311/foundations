{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites: Hessian Matrix, Definiteness, and Curvature\n",
    "\n",
    "This notebook covers essential background knowledge needed before studying Chapter 7.1 of Deep Learning.\n",
    "\n",
    "**Topics covered:**\n",
    "1. Hessian Matrix - Definition and Properties\n",
    "2. Matrix Definiteness - Classification and Testing\n",
    "3. Curvature Interpretation - Geometric Understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### **Why Second Derivatives Matter in Optimization**\n\nWhen minimizing a function $f(x)$:\n\n1. **First derivative $f'(x) = 0$** → Find critical points (potential minimum/maximum)\n2. **Second derivative $f''(x)$** → Determine the **type** of critical point:\n   - $f''(x) > 0$ → **Local minimum** ✅\n   - $f''(x) < 0$ → **Local maximum** ❌\n   - $f''(x) = 0$ → **Inconclusive** (need further testing)\n\n**In multiple dimensions:**\n- The **Hessian matrix** generalizes the second derivative to functions of many variables\n- It's an $n \\times n$ matrix of all second-order partial derivatives\n- We use **eigenvalues** of the Hessian (instead of sign of $f''$) to classify critical points\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "x = np.linspace(-3, 3, 200)\n\n# Three functions with different second derivatives\nf1 = x**2           # f''(x) = 2 (positive, curves up)\nf2 = -x**2          # f''(x) = -2 (negative, curves down)\nf3 = x**3           # f''(x) = 6x (changes sign at x=0)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Function 1: f(x) = x²\naxes[0].plot(x, f1, 'b-', linewidth=2)\naxes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0].set_title(\"f(x) = x²\\nf''(x) = 2 > 0\\n(Curves UP)\", fontsize=11)\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('f(x)')\naxes[0].grid(True, alpha=0.3)\naxes[0].annotate('Minimum', xy=(0, 0), xytext=(0.5, 2),\n                arrowprops=dict(arrowstyle='->', color='red'),\n                fontsize=10, color='red')\n\n# Function 2: f(x) = -x²\naxes[1].plot(x, f2, 'r-', linewidth=2)\naxes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1].set_title(\"f(x) = -x²\\nf''(x) = -2 < 0\\n(Curves DOWN)\", fontsize=11)\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('f(x)')\naxes[1].grid(True, alpha=0.3)\naxes[1].annotate('Maximum', xy=(0, 0), xytext=(0.5, -2),\n                arrowprops=dict(arrowstyle='->', color='red'),\n                fontsize=10, color='red')\n\n# Function 3: f(x) = x³\naxes[2].plot(x, f3, 'g-', linewidth=2)\naxes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[2].set_title(\"f(x) = x³\\nf''(x) = 6x\\n(Changes sign)\", fontsize=11)\naxes[2].set_xlabel('x')\naxes[2].set_ylabel('f(x)')\naxes[2].grid(True, alpha=0.3)\naxes[2].annotate('Inflection point', xy=(0, 0), xytext=(1, -10),\n                arrowprops=dict(arrowstyle='->', color='red'),\n                fontsize=10, color='red')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key observations:\")\nprint(\"1. f''(x) > 0 → Curves upward (bowl shape) → potential minimum\")\nprint(\"2. f''(x) < 0 → Curves downward (dome shape) → potential maximum\")\nprint(\"3. f''(x) = 0 → Could be inflection point (curvature changes)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Example: Visualizing Second Derivatives**\n\nLet's examine three functions with different curvatures:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 0️⃣ Quick Review: Second Derivatives\n\n### **What is a Second Derivative?**\n\nThe **second derivative** is the derivative of the derivative:\n\n$$\nf'(x) = \\frac{df}{dx} \\quad \\text{(first derivative)}\n$$\n\n$$\nf''(x) = \\frac{d^2f}{dx^2} = \\frac{d}{dx}\\left(\\frac{df}{dx}\\right) \\quad \\text{(second derivative)}\n$$\n\n### **What Does It Tell Us?**\n\n| Derivative | Measures | Interpretation |\n|------------|----------|----------------|\n| **First** $f'(x)$ | Rate of change | How fast $f$ is increasing/decreasing |\n| **Second** $f''(x)$ | Rate of change of the rate of change | **Curvature** (how the slope is changing) |\n\n### **Geometric Meaning**\n\n- $f''(x) > 0$ → Function is **curving upward** (convex, like a bowl ∪)\n- $f''(x) < 0$ → Function is **curving downward** (concave, like a dome ∩)\n- $f''(x) = 0$ → **Inflection point** (curvature changes direction)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1️⃣ Hessian Matrix\n\n### **Definition**\nFor a scalar-valued function  \n$$\nf(\\mathbf{x}) = f(x_1, x_2, \\dots, x_n)\n$$  \nthe **Hessian matrix** is the square matrix of all second-order partial derivatives:\n\n$$\nH(f) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n$$\n\n### **Key Properties**\n- If mixed partial derivatives are continuous,  \n  $$\n  \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\n  $$\n  → the Hessian is **symmetric**: $H = H^\\top$.\n- Shape: $n \\times n$ (determined by number of variables, not number of terms)\n- Describes **curvature** (how the function bends) in all directions."
  },
  {
   "cell_type": "markdown",
   "source": "### **Simple Example**\n$$\nf(x, y) = x^2 + 3y^2\n$$\n\n$$\nH =\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 6\n\\end{bmatrix}\n$$\n- Along x-axis: curvature = 2  \n- Along y-axis: curvature = 6  \n- No cross-dependency (off-diagonal = 0)\n\n### **Usage**\n- **Optimization:** Identify minima, maxima, or saddle points  \n- **Second-order methods:** Newton's method  \n- **Geometry:** Describes the \"shape\" of the loss surface  \n- **Machine learning:** Analyze curvature of loss for stability and convergence",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Hessian for f(x,y) = x² + xy + y²\nH_example = np.array([[2, 1],\n                      [1, 2]])\n\nprint(\"Original Hessian:\")\nprint(H_example)\nprint(\"\\n\" + \"=\"*60)\n\n# Step 1: Compute eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(H_example)\n\nprint(\"\\nStep 1: Compute eigenvalues and eigenvectors\")\nprint(\"-\"*60)\nprint(f\"Eigenvalues (λ): {eigenvalues}\")\nprint(f\"\\nEigenvectors (columns of Q):\")\nprint(eigenvectors)\nprint(f\"\\nInterpretation:\")\nprint(f\"  - λ₁ = {eigenvalues[0]:.1f}: curvature along direction [{eigenvectors[0,0]:.3f}, {eigenvectors[1,0]:.3f}]\")\nprint(f\"  - λ₂ = {eigenvalues[1]:.1f}: curvature along direction [{eigenvectors[0,1]:.3f}, {eigenvectors[1,1]:.3f}]\")\n\n# Step 2: Build Q and Λ matrices\nQ = eigenvectors\nLambda = np.diag(eigenvalues)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nStep 2: Build Q (eigenvector matrix) and Λ (diagonal eigenvalue matrix)\")\nprint(\"-\"*60)\nprint(\"Q (eigenvectors as columns):\")\nprint(Q)\nprint(\"\\nΛ (eigenvalues on diagonal):\")\nprint(Lambda)\n\n# Step 3: Verify Q is orthogonal (Q^T Q = I)\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nStep 3: Verify Q is orthogonal\")\nprint(\"-\"*60)\nQ_T_Q = Q.T @ Q\nprint(\"Q^T Q =\")\nprint(Q_T_Q)\nprint(f\"Is Q^T Q ≈ I? {np.allclose(Q_T_Q, np.eye(2))}\")\n\n# Step 4: Reconstruct H from Q Λ Q^T\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nStep 4: Reconstruct H = Q Λ Q^T\")\nprint(\"-\"*60)\nH_reconstructed = Q @ Lambda @ Q.T\nprint(\"Q Λ Q^T =\")\nprint(H_reconstructed)\nprint(f\"\\nDoes H = Q Λ Q^T? {np.allclose(H_example, H_reconstructed)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\n🎯 Key Insight:\")\nprint(\"The eigenvalue decomposition transforms H into a diagonal form Λ\")\nprint(\"where curvatures along different principal directions are decoupled!\")\nprint(f\"  - Direction 1: curvature = {eigenvalues[0]:.1f}\")\nprint(f\"  - Direction 2: curvature = {eigenvalues[1]:.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Step-by-Step Example: Eigenvalue Decomposition\n\nLet's compute $H = Q\\Lambda Q^T$ for $f(x,y) = x^2 + xy + y^2$ with Hessian $H = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### **Eigenvalue Decomposition of Symmetric Hessian**\n\nSince the Hessian is **symmetric** (under smooth conditions), it has a special decomposition:\n\n$$\nH = Q \\Lambda Q^T\n$$\n\nwhere:\n- $Q$: orthogonal matrix of **eigenvectors** (principal directions)\n- $\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)$: diagonal matrix of **eigenvalues** (curvatures along principal directions)\n- $Q^T Q = I$ (orthonormal eigenvectors)\n\n**Geometric interpretation:**\n- Eigenvectors point along the **principal axes** of curvature\n- Eigenvalues give the **amount of curvature** in each principal direction\n- This decomposition \"diagonalizes\" the Hessian, revealing independent directions\n\n**Why this matters:**\n- Optimization converges fastest when all eigenvalues are similar (well-conditioned)\n- Large spread in eigenvalues → slow convergence (ill-conditioned)\n- Zero eigenvalues → flat directions (semi-definite)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Computing a Hessian Matrix\n",
    "\n",
    "Let's compute the Hessian for $f(x, y) = x^2 + 3y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    \"\"\"Function: f(x,y) = x^2 + 3y^2\"\"\"\n",
    "    return x**2 + 3*y**2\n",
    "\n",
    "def hessian_f1():\n",
    "    \"\"\"\n",
    "    Compute Hessian of f(x,y) = x^2 + 3y^2\n",
    "    \n",
    "    First derivatives:\n",
    "    ∂f/∂x = 2x\n",
    "    ∂f/∂y = 6y\n",
    "    \n",
    "    Second derivatives:\n",
    "    ∂²f/∂x² = 2\n",
    "    ∂²f/∂y² = 6\n",
    "    ∂²f/∂x∂y = 0\n",
    "    ∂²f/∂y∂x = 0\n",
    "    \"\"\"\n",
    "    return np.array([[2, 0],\n",
    "                     [0, 6]])\n",
    "\n",
    "H1 = hessian_f1()\n",
    "print(\"Hessian of f(x,y) = x² + 3y²:\")\n",
    "print(H1)\n",
    "print(f\"\\nSymmetric: {np.allclose(H1, H1.T)}\")\n",
    "print(f\"Eigenvalues: {np.linalg.eigvals(H1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Hessian with Cross Terms\n",
    "\n",
    "$$f(x, y) = x^2 + xy + y^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x, y):\n",
    "    \"\"\"Function: f(x,y) = x^2 + xy + y^2\"\"\"\n",
    "    return x**2 + x*y + y**2\n",
    "\n",
    "def hessian_f2():\n",
    "    \"\"\"\n",
    "    Compute Hessian of f(x,y) = x^2 + xy + y^2\n",
    "    \n",
    "    First derivatives:\n",
    "    ∂f/∂x = 2x + y\n",
    "    ∂f/∂y = x + 2y\n",
    "    \n",
    "    Second derivatives:\n",
    "    ∂²f/∂x² = 2\n",
    "    ∂²f/∂y² = 2\n",
    "    ∂²f/∂x∂y = 1\n",
    "    ∂²f/∂y∂x = 1\n",
    "    \"\"\"\n",
    "    return np.array([[2, 1],\n",
    "                     [1, 2]])\n",
    "\n",
    "H2 = hessian_f2()\n",
    "print(\"Hessian of f(x,y) = x² + xy + y²:\")\n",
    "print(H2)\n",
    "print(f\"\\nSymmetric: {np.allclose(H2, H2.T)}\")\n",
    "print(f\"Eigenvalues: {np.linalg.eigvals(H2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Strong Correlation Between Variables\n",
    "\n",
    "$$f(x, y) = x^2 + 2xy + y^2 = (x+y)^2$$\n",
    "\n",
    "This function shows **strong correlation** between $x$ and $y$ due to the large cross-term $2xy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x, y):\n",
    "    \"\"\"Function: f(x,y) = x^2 + 2xy + y^2 = (x+y)^2\"\"\"\n",
    "    return x**2 + 2*x*y + y**2\n",
    "\n",
    "def hessian_f3():\n",
    "    \"\"\"\n",
    "    Compute Hessian of f(x,y) = x^2 + 2xy + y^2\n",
    "    \n",
    "    First derivatives:\n",
    "    ∂f/∂x = 2x + 2y\n",
    "    ∂f/∂y = 2x + 2y\n",
    "    \n",
    "    Second derivatives:\n",
    "    ∂²f/∂x² = 2\n",
    "    ∂²f/∂y² = 2\n",
    "    ∂²f/∂x∂y = 2\n",
    "    ∂²f/∂y∂x = 2\n",
    "    \"\"\"\n",
    "    return np.array([[2, 2],\n",
    "                     [2, 2]])\n",
    "\n",
    "H3 = hessian_f3()\n",
    "print(\"Hessian of f(x,y) = x² + 2xy + y² = (x+y)²:\")\n",
    "print(H3)\n",
    "print(f\"\\nSymmetric: {np.allclose(H3, H3.T)}\")\n",
    "print(f\"Determinant: {np.linalg.det(H3):.10f}\")\n",
    "eigenvalues = np.linalg.eigvals(H3)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "\n",
    "# Classify definiteness\n",
    "def_type, _ = classify_definiteness(H3)\n",
    "print(f\"\\nDefiniteness: {def_type}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- One eigenvalue = 0 → Function is FLAT along one direction\")\n",
    "print(\"- One eigenvalue = 4 → Function curves UP in perpendicular direction\")\n",
    "print(\"- This creates a 'valley' or 'ridge' structure\")\n",
    "print(\"- Variables x and y are PERFECTLY CORRELATED in this function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlated function\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-2, 2, 100)\n",
    "X_corr, Y_corr = np.meshgrid(x_range, y_range)\n",
    "Z_corr = f3(X_corr, Y_corr)\n",
    "\n",
    "surf = ax1.plot_surface(X_corr, Y_corr, Z_corr, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = (x+y)² - Valley Structure')\n",
    "ax1.view_init(elev=25, azim=45)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contour = ax2.contour(X_corr, Y_corr, Z_corr, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.plot(0, 0, 'r*', markersize=15, label='Critical point (0,0)')\n",
    "\n",
    "# Show the flat direction (eigenvalue = 0): x + y = 0\n",
    "x_line = np.linspace(-2, 2, 100)\n",
    "y_line = -x_line  # x + y = 0\n",
    "ax2.plot(x_line, y_line, 'b--', linewidth=2, label='Flat direction (x+y=0)')\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour: Note the valley along x+y=0')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"1. The function is FLAT (zero curvature) along the line x+y=0\")\n",
    "print(\"2. The function curves UP perpendicular to this line\")\n",
    "print(\"3. This creates a 'valley' or 'trough' structure\")\n",
    "print(\"4. In deep learning, this represents strong CORRELATION between parameters\")\n",
    "print(\"5. Optimization can be slow along the valley - need adaptive methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Matrix Definiteness\n",
    "\n",
    "For a **symmetric matrix** $A$, its definiteness is determined by the **signs of its eigenvalues**.\n",
    "\n",
    "| Type | Eigenvalues | Condition on $x^T A x$ | Interpretation |\n",
    "|------|--------------|---------------------------|----------------|\n",
    "| **Positive definite (PD)** | all $>0$ | $x^T A x > 0$ for all $x\\neq0$ | Always curves **upward** |\n",
    "| **Negative definite (ND)** | all $<0$ | $x^T A x < 0$ for all $x\\neq0$ | Always curves **downward** |\n",
    "| **Indefinite** | some $+$, some $-$ | depends on direction | Curves **up** in some directions, **down** in others |\n",
    "| **Positive semi-definite (PSD)** | all $\\ge 0$ | $x^T A x \\ge 0$ | Flat in some directions, curves up in others |\n",
    "| **Negative semi-definite (NSD)** | all $\\le 0$ | $x^T A x \\le 0$ | Flat in some directions, curves down in others |\n",
    "\n",
    "**Quick Test (2×2 case):**\n",
    "For $A = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}$:\n",
    "- PD if $a>0$ and $ac-b^2>0$\n",
    "- ND if $a<0$ and $ac-b^2>0$\n",
    "- Indefinite if $ac-b^2<0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_definiteness(A):\n",
    "    \"\"\"\n",
    "    Classify the definiteness of a symmetric matrix based on its eigenvalues.\n",
    "    \"\"\"\n",
    "    eigenvalues = np.linalg.eigvals(A)\n",
    "    \n",
    "    if np.all(eigenvalues > 1e-10):  # all positive (with tolerance)\n",
    "        return \"Positive Definite (PD)\", eigenvalues\n",
    "    elif np.all(eigenvalues < -1e-10):  # all negative\n",
    "        return \"Negative Definite (ND)\", eigenvalues\n",
    "    elif np.all(eigenvalues >= -1e-10):  # all non-negative\n",
    "        return \"Positive Semi-Definite (PSD)\", eigenvalues\n",
    "    elif np.all(eigenvalues <= 1e-10):  # all non-positive\n",
    "        return \"Negative Semi-Definite (NSD)\", eigenvalues\n",
    "    else:  # mixed signs\n",
    "        return \"Indefinite\", eigenvalues\n",
    "\n",
    "# Test different matrices\n",
    "matrices = {\n",
    "    \"H1 (x²+3y²)\": H1,\n",
    "    \"H2 (x²+xy+y²)\": H2,\n",
    "    \"H3 (x²+2xy+y²)\": H3,\n",
    "    \"Negative definite\": np.array([[-2, 0], [0, -3]]),\n",
    "    \"Indefinite (saddle)\": np.array([[2, 0], [0, -2]]),\n",
    "    \"PSD (flat direction)\": np.array([[2, 0], [0, 0]])\n",
    "}\n",
    "\n",
    "print(\"Matrix Definiteness Classification:\")\n",
    "print(\"=\"*60)\n",
    "for name, matrix in matrices.items():\n",
    "    def_type, eigs = classify_definiteness(matrix)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Type: {def_type}\")\n",
    "    print(f\"  Eigenvalues: {eigs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Interpreting Hessian Definiteness and Surface Shape\n",
    "\n",
    "The **Hessian matrix** determines how a scalar function behaves near a critical point (where the gradient = 0).\n",
    "\n",
    "| Hessian Type | Eigenvalues | Surface Shape | Interpretation |\n",
    "|---------------|--------------|----------------|----------------|\n",
    "| **Positive definite** | all positive | 🟢 Bowl (convex up) | Local minimum |\n",
    "| **Negative definite** | all negative | 🔴 Dome (concave down) | Local maximum |\n",
    "| **Indefinite** | mixed signs | ⚫ Saddle | Neither min nor max |\n",
    "| **Positive semi-definite** | ≥ 0 | ⚪ Flat-bottom bowl | Possibly minimum, but flat in some directions |\n",
    "| **Negative semi-definite** | ≤ 0 | ⚪ Flat-top dome | Possibly maximum, but flat in some directions |\n",
    "\n",
    "### **Visualization Intuition**\n",
    "- $H = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}$: round bowl → **minimum**  \n",
    "- $H = \\begin{bmatrix} -2 & 0 \\\\ 0 & -2 \\end{bmatrix}$: round hill → **maximum**  \n",
    "- $H = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}$: saddle surface → **neither**  \n",
    "- $H = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0 \\end{bmatrix}$: flat along one axis → **semi-definite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Different Surface Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for plotting\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Define different functions with different Hessian types\n",
    "def positive_definite(x, y):\n",
    "    \"\"\"Minimum: f = x² + y²\"\"\"\n",
    "    return x**2 + y**2\n",
    "\n",
    "def negative_definite(x, y):\n",
    "    \"\"\"Maximum: f = -x² - y²\"\"\"\n",
    "    return -x**2 - y**2\n",
    "\n",
    "def indefinite(x, y):\n",
    "    \"\"\"Saddle: f = x² - y²\"\"\"\n",
    "    return x**2 - y**2\n",
    "\n",
    "def semi_definite(x, y):\n",
    "    \"\"\"Flat direction: f = x²\"\"\"\n",
    "    return x**2\n",
    "\n",
    "# Create subplots\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "functions = [\n",
    "    (positive_definite, \"Positive Definite\\n(Bowl - Minimum)\", \"Greens\"),\n",
    "    (negative_definite, \"Negative Definite\\n(Dome - Maximum)\", \"Reds\"),\n",
    "    (indefinite, \"Indefinite\\n(Saddle Point)\", \"RdBu\"),\n",
    "    (semi_definite, \"Semi-Definite\\n(Flat Direction)\", \"YlOrRd\")\n",
    "]\n",
    "\n",
    "for idx, (func, title, cmap) in enumerate(functions, 1):\n",
    "    ax = fig.add_subplot(1, 4, idx, projection='3d')\n",
    "    Z = func(X, Y)\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8, \n",
    "                           linewidth=0, antialiased=True)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('f(x,y)')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.view_init(elev=25, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Plots for Better Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for idx, (func, title, cmap) in enumerate(functions):\n",
    "    ax = axes[idx]\n",
    "    Z = func(X, Y)\n",
    "    \n",
    "    contour = ax.contour(X, Y, Z, levels=15, cmap=cmap)\n",
    "    ax.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Mark the critical point at origin\n",
    "    ax.plot(0, 0, 'r*', markersize=15, label='Critical point')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Contour interpretation:\")\n",
    "print(\"- Positive Definite: Concentric circles/ellipses around minimum\")\n",
    "print(\"- Negative Definite: Concentric circles/ellipses around maximum\")\n",
    "print(\"- Indefinite: Hyperbolic contours (saddle point)\")\n",
    "print(\"- Semi-Definite: Parallel lines (flat in one direction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Summary\n",
    "\n",
    "### Key Relationships\n",
    "\n",
    "| Concept | Order | What it tells us |\n",
    "|---------|-------|------------------|\n",
    "| **Gradient** (Jacobian) | 1st order | Direction of **steepest ascent** |\n",
    "| **Hessian** | 2nd order | **Curvature** (how gradient changes) |\n",
    "| **Eigenvalues of Hessian** | - | Type of curvature (up/down/mixed) |\n",
    "| **Definiteness** | - | Nature of critical point (min/max/saddle) |\n",
    "\n",
    "### Decision Tree for Critical Points\n",
    "\n",
    "```\n",
    "At critical point (∇f = 0):\n",
    "    |\n",
    "    ├─ All eigenvalues > 0  →  Positive Definite  →  LOCAL MINIMUM\n",
    "    |\n",
    "    ├─ All eigenvalues < 0  →  Negative Definite  →  LOCAL MAXIMUM\n",
    "    |\n",
    "    ├─ Mixed signs          →  Indefinite         →  SADDLE POINT\n",
    "    |\n",
    "    └─ Some zero eigenvalues → Semi-definite     →  TEST FURTHER\n",
    "```\n",
    "\n",
    "### Practical Implications for Deep Learning\n",
    "\n",
    "1. **Positive definite Hessian** → Loss surface is **convex** locally → guaranteed to find minimum\n",
    "2. **Negative definite Hessian** → Unstable (at a maximum) → need to escape\n",
    "3. **Indefinite Hessian** → **Saddle point** → common in high-dimensional spaces\n",
    "4. **Large eigenvalues** → **Steep curvature** → small learning rate needed\n",
    "5. **Small eigenvalues** → **Flat region** → slow convergence\n",
    "\n",
    "---\n",
    "\n",
    "You are now ready for **Chapter 7.1: Optimization for Training Deep Models**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Connection to Deep Learning Practice\n",
    "\n",
    "### Local vs Global Minima\n",
    "\n",
    "**Important distinction:**\n",
    "- **Bowl (Positive Definite Hessian)** = **Local minimum** (lowest point in a neighborhood)\n",
    "- **Global minimum** = Absolute lowest point across the entire loss surface\n",
    "\n",
    "```\n",
    "Deep Learning Reality:\n",
    "    ┌─────────────────────────────────────────────┐\n",
    "    │  Loss Surface = High-dimensional landscape  │\n",
    "    │  with MANY local minima                     │\n",
    "    └─────────────────────────────────────────────┘\n",
    "              ↓\n",
    "    We rarely find THE global minimum\n",
    "    We find A GOOD local minimum (good enough!)\n",
    "```\n",
    "\n",
    "### Why \"Good Enough\" Local Minima Work\n",
    "\n",
    "In deep learning, we **don't need the global minimum** because:\n",
    "\n",
    "1. **Generalization matters more than perfect training loss**\n",
    "   - A slightly higher training loss often generalizes better\n",
    "   - Overfitting happens when we optimize training loss too much\n",
    "\n",
    "2. **Many local minima perform similarly well**\n",
    "   - Modern neural networks have many \"good\" local minima\n",
    "   - Empirically, SGD finds solutions that generalize well\n",
    "\n",
    "3. **The journey matters**\n",
    "   - How we arrive at the minimum (via SGD with noise) provides implicit regularization\n",
    "   - This helps avoid overfitting\n",
    "\n",
    "### Saddle Points: The Real Challenge\n",
    "\n",
    "**In high-dimensional spaces (neural networks):**\n",
    "\n",
    "```\n",
    "Problem: Most critical points are SADDLE POINTS, not local minima!\n",
    "\n",
    "Why?\n",
    "- In n-dimensional space, need ALL n eigenvalues > 0 for minimum\n",
    "- Probability of this decreases exponentially with dimension\n",
    "- More likely to have mixed signs → saddle point\n",
    "```\n",
    "\n",
    "**What happens at a saddle point:**\n",
    "- Gradient = 0 (seems like we're stuck!)\n",
    "- But: Some directions curve up, some down\n",
    "- SGD's noise helps escape by exploring different directions\n",
    "- Momentum-based optimizers help \"push through\" saddle points\n",
    "\n",
    "### Curvature and Learning Rate\n",
    "\n",
    "The Hessian's eigenvalues directly affect optimization:\n",
    "\n",
    "| Scenario | Eigenvalues | What happens | Solution |\n",
    "|----------|-------------|--------------|----------|\n",
    "| **Steep curvature** | Large positive | Gradient descent overshoots | Use **small learning rate** |\n",
    "| **Flat region** | Small positive | Very slow progress | Use **large learning rate** or momentum |\n",
    "| **Different directions** | Mixed magnitudes | Some directions converge fast, others slow | **Adaptive learning rates** |\n",
    "| **Saddle point** | Mixed signs | Gradient vanishes but not at minimum | **Momentum** or noise helps escape |\n",
    "\n",
    "### Practical Takeaways\n",
    "\n",
    "1. **Finding a bowl ≠ Best solution**\n",
    "   - It's a local minimum, might not be global\n",
    "   - But often \"good enough\" for practical purposes\n",
    "\n",
    "2. **Saddle points are common**\n",
    "   - Don't panic if training slows down temporarily\n",
    "   - SGD noise and momentum help escape\n",
    "\n",
    "3. **Curvature varies across loss surface**\n",
    "   - Adaptive optimizers handle this automatically\n",
    "   - Learning rate schedules help navigate different regions\n",
    "\n",
    "4. **High-dimensional intuition breaks down**\n",
    "   - What seems like a minimum in 2D visualization\n",
    "   - Might be a saddle point in 1000D parameter space\n",
    "\n",
    "5. **We optimize for generalization, not perfect training loss**\n",
    "   - Stopping before reaching the absolute minimum often helps\n",
    "   - Early stopping, regularization prevent over-optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Key insight for Chapter 7:** Understanding curvature helps explain *why* certain optimization algorithms work and *when* to use them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}